import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import streamlit as st
import json

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('russian'))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É
def load_data():
    try:
        with open('data/info_data.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        st.error("–§–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return {}

def preprocess_text(text):
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = word_tokenize(text.lower())
    processed_tokens = []
    for token in tokens:
        if token.isalpha() and token not in stop_words:
            lemma = lemmatizer.lemmatize(token)
            processed_tokens.append(lemma)
    return processed_tokens

def find_answer(question, knowledge_base):
    processed_question = preprocess_text(question)

    best_match = None
    max_score = 0

    for category, subcategories in knowledge_base.items():
        for subcategory, data in subcategories.items():
            keywords = data.get('keywords', [])
            score = sum(1 for keyword in keywords if keyword in processed_question)

            if score > max_score:
                max_score = score
                best_match = data.get('response')

    return best_match if max_score > 0 else "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –ø–æ–Ω—è–ª –≤–∞—à –≤–æ–ø—Ä–æ—Å."

st.set_page_config(page_title="Sagynbekovvv K", page_icon="üòâ")

st.title("–ß–∞—Ç-–±–æ—Ç")
st.markdown("")

knowledge_base = load_data()

prompt = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ö–∞–∫–∏–µ –ø–∞—Ä—ã —Å–µ–≥–æ–¥–Ω—è?")

if prompt:
    response = find_answer(prompt, knowledge_base)

    st.markdown("### –û—Ç–≤–µ—Ç:")
    st.info(response)

st.markdown("---")
st.markdown("Avtor: Sagynbekovvv K ‚ù§Ô∏è")
