import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import streamlit as st
import json

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã NLP
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('russian'))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
def load_data():
    try:
        with open('data/info_data.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        st.error("–§–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return {}

# –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
def preprocess_text(text):
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = word_tokenize(text.lower())
    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    processed_tokens = []
    for token in tokens:
        if token.isalpha() and token not in stop_words:
            lemma = lemmatizer.lemmatize(token)
            processed_tokens.append(lemma)
    return processed_tokens

def find_answer(question, knowledge_base):
    processed_question = preprocess_text(question)

    best_match = None
    max_score = 0

    # –ò—â–µ–º –ª—É—á—à–∏–π match –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    for category, subcategories in knowledge_base.items():
        for subcategory, data in subcategories.items():
            keywords = data.get('keywords', [])
            # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            score = sum(1 for keyword in keywords if keyword in processed_question)

            if score > max_score:
                max_score = score
                best_match = data.get('response')

    return best_match if max_score > 0 else "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –ø–æ–Ω—è–ª –≤–∞—à –≤–æ–ø—Ä–æ—Å."

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Streamlit
st.set_page_config(page_title="Sagynbekovvv K", page_icon="üòâ")

st.title("–ß–∞—Ç-–±–æ—Ç")
st.markdown("")

knowledge_base = load_data()

prompt = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ö–∞–∫–∏–µ –ø–∞—Ä—ã —Å–µ–≥–æ–¥–Ω—è?")

if prompt:
    response = find_answer(prompt, knowledge_base)

    st.markdown("### –û—Ç–≤–µ—Ç:")
    st.info(response)

st.markdown("---")
st.markdown("Avtor: Sagynbekovvv K ‚ù§Ô∏è")